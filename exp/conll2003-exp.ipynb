{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef8ed999-1fec-4fca-8c25-537528543379",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lamuguo/venv/dbpedia/lib/python3.12/site-packages/datasets/load.py:1454: FutureWarning: The repository for conll2003 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/conll2003\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 加载 CoNLL 2003 数据集\n",
    "dataset = load_dataset(\"conll2003\")\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "\n",
    "# 设置设备，检查是否可以使用 GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10ea7e7f-281c-45da-ae76-198cc2184f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# 加载预训练的 BERT 模型和 tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-cased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6610cf21-c3dc-4450-b2d3-abc51500f166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in dataset: {0, 1, 2, 3, 4, 5, 6, 7, 8}\n",
      "Number of unique labels: 9\n"
     ]
    }
   ],
   "source": [
    "# 查看训练数据集中有多少个独特的标签\n",
    "all_labels = []\n",
    "for batch in train_dataset:\n",
    "    all_labels.extend(batch['ner_tags'])\n",
    "\n",
    "unique_labels = set(all_labels)\n",
    "print(f\"Unique labels in dataset: {unique_labels}\")\n",
    "print(f\"Number of unique labels: {len(unique_labels)}\")\n",
    "num_labels = len(unique_labels)  # 将 num_labels 设置为实际标签数量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f320eb3-7d0d-4cf3-88e8-8b921c25e695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "class SimpleNERModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_labels):\n",
    "        super(SimpleNERModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)  # 第一个全连接层（中间层）\n",
    "        self.relu = nn.ReLU()  # ReLU 激活函数\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_labels)  # 输出层\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        x = self.fc1(embeddings)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 初始化 BERT 模型和 tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-cased').to(device)\n",
    "\n",
    "# 加载数据\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"conll2003\")\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "# 模型参数\n",
    "embedding_dim = 768  # BERT embedding 的维度\n",
    "hidden_dim = 128  # 中间层的维度\n",
    "\n",
    "# 初始化模型\n",
    "model = SimpleNERModel(embedding_dim, hidden_dim, num_labels).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef97f354-7dfe-4214-93fa-74bd2216f452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1, Loss: 0.7691693902015686\n",
      "Epoch 1, Batch 2, Loss: 0.7697692513465881\n",
      "Epoch 1, Batch 3, Loss: 0.7938210368156433\n",
      "Epoch 1, Batch 4, Loss: 0.7496778964996338\n",
      "Epoch 1, Batch 5, Loss: 0.7899244427680969\n",
      "Epoch 1, Batch 6, Loss: 0.7413403987884521\n",
      "Epoch 1, Batch 7, Loss: 0.7822661995887756\n",
      "Epoch 1, Batch 8, Loss: 0.7747786045074463\n",
      "Epoch 1, Batch 9, Loss: 0.7699350714683533\n",
      "Epoch 1, Batch 10, Loss: 0.7893337607383728\n",
      "Epoch 1, Batch 11, Loss: 0.7792841196060181\n",
      "Epoch 1, Batch 12, Loss: 0.7833759188652039\n",
      "Epoch 1, Batch 13, Loss: 0.7742283344268799\n",
      "Epoch 1, Batch 14, Loss: 0.7822579145431519\n",
      "Epoch 1 completed.\n",
      "Epoch 2, Batch 1, Loss: 0.7663796544075012\n",
      "Epoch 2, Batch 2, Loss: 0.7588261365890503\n",
      "Epoch 2, Batch 3, Loss: 0.7753508687019348\n",
      "Epoch 2, Batch 4, Loss: 0.7632139921188354\n",
      "Epoch 2, Batch 5, Loss: 0.762175440788269\n",
      "Epoch 2, Batch 6, Loss: 0.7678371071815491\n",
      "Epoch 2, Batch 7, Loss: 0.782738447189331\n",
      "Epoch 2, Batch 8, Loss: 0.7580418586730957\n",
      "Epoch 2, Batch 9, Loss: 0.7732134461402893\n",
      "Epoch 2, Batch 10, Loss: 0.7862698435783386\n",
      "Epoch 2, Batch 11, Loss: 0.7983150482177734\n",
      "Epoch 2, Batch 12, Loss: 0.7630063891410828\n",
      "Epoch 2, Batch 13, Loss: 0.7665370106697083\n",
      "Epoch 2, Batch 14, Loss: 0.7361589670181274\n",
      "Epoch 2 completed.\n",
      "Epoch 3, Batch 1, Loss: 0.7703577876091003\n",
      "Epoch 3, Batch 2, Loss: 0.7627850770950317\n",
      "Epoch 3, Batch 3, Loss: 0.7917178273200989\n",
      "Epoch 3, Batch 4, Loss: 0.79046630859375\n",
      "Epoch 3, Batch 5, Loss: 0.7614166140556335\n",
      "Epoch 3, Batch 6, Loss: 0.7483991384506226\n",
      "Epoch 3, Batch 7, Loss: 0.7588467001914978\n",
      "Epoch 3, Batch 8, Loss: 0.7662821412086487\n",
      "Epoch 3, Batch 9, Loss: 0.7623776793479919\n",
      "Epoch 3, Batch 10, Loss: 0.7615205645561218\n",
      "Epoch 3, Batch 11, Loss: 0.7474998831748962\n",
      "Epoch 3, Batch 12, Loss: 0.759986162185669\n",
      "Epoch 3, Batch 13, Loss: 0.7470837235450745\n",
      "Epoch 3, Batch 14, Loss: 0.7599362134933472\n",
      "Epoch 3 completed.\n",
      "Epoch 4, Batch 1, Loss: 0.7700403928756714\n",
      "Epoch 4, Batch 2, Loss: 0.7664452791213989\n",
      "Epoch 4, Batch 3, Loss: 0.7577154040336609\n",
      "Epoch 4, Batch 4, Loss: 0.7613972425460815\n",
      "Epoch 4, Batch 5, Loss: 0.7379927039146423\n",
      "Epoch 4, Batch 6, Loss: 0.7644079327583313\n",
      "Epoch 4, Batch 7, Loss: 0.7384775280952454\n",
      "Epoch 4, Batch 8, Loss: 0.7429207563400269\n",
      "Epoch 4, Batch 9, Loss: 0.7358849048614502\n",
      "Epoch 4, Batch 10, Loss: 0.7487062811851501\n",
      "Epoch 4, Batch 11, Loss: 0.7818589806556702\n",
      "Epoch 4, Batch 12, Loss: 0.7842581272125244\n",
      "Epoch 4, Batch 13, Loss: 0.7522426247596741\n",
      "Epoch 4, Batch 14, Loss: 0.781982421875\n",
      "Epoch 4 completed.\n",
      "Epoch 5, Batch 1, Loss: 0.7556779384613037\n",
      "Epoch 5, Batch 2, Loss: 0.7410240173339844\n",
      "Epoch 5, Batch 3, Loss: 0.7770952582359314\n",
      "Epoch 5, Batch 4, Loss: 0.744880735874176\n",
      "Epoch 5, Batch 5, Loss: 0.7488576769828796\n",
      "Epoch 5, Batch 6, Loss: 0.748772144317627\n",
      "Epoch 5, Batch 7, Loss: 0.7417492866516113\n",
      "Epoch 5, Batch 8, Loss: 0.7364426851272583\n",
      "Epoch 5, Batch 9, Loss: 0.7528979182243347\n",
      "Epoch 5, Batch 10, Loss: 0.7448751330375671\n",
      "Epoch 5, Batch 11, Loss: 0.7346433401107788\n",
      "Epoch 5, Batch 12, Loss: 0.7723400592803955\n",
      "Epoch 5, Batch 13, Loss: 0.7677158713340759\n",
      "Epoch 5, Batch 14, Loss: 0.7861829400062561\n",
      "Epoch 5 completed.\n",
      "Epoch 6, Batch 1, Loss: 0.7347887754440308\n",
      "Epoch 6, Batch 2, Loss: 0.7773791551589966\n",
      "Epoch 6, Batch 3, Loss: 0.7743324041366577\n",
      "Epoch 6, Batch 4, Loss: 0.7524611949920654\n",
      "Epoch 6, Batch 5, Loss: 0.7604318857192993\n",
      "Epoch 6, Batch 6, Loss: 0.7424443364143372\n",
      "Epoch 6, Batch 7, Loss: 0.7191789150238037\n",
      "Epoch 6, Batch 8, Loss: 0.7560359835624695\n",
      "Epoch 6, Batch 9, Loss: 0.7527438402175903\n",
      "Epoch 6, Batch 10, Loss: 0.7395263910293579\n",
      "Epoch 6, Batch 11, Loss: 0.7390776872634888\n",
      "Epoch 6, Batch 12, Loss: 0.747645378112793\n",
      "Epoch 6, Batch 13, Loss: 0.7315791249275208\n",
      "Epoch 6, Batch 14, Loss: 0.7468459010124207\n",
      "Epoch 6 completed.\n",
      "Epoch 7, Batch 1, Loss: 0.7400032877922058\n",
      "Epoch 7, Batch 2, Loss: 0.7172189354896545\n",
      "Epoch 7, Batch 3, Loss: 0.7054667472839355\n",
      "Epoch 7, Batch 4, Loss: 0.7287432551383972\n",
      "Epoch 7, Batch 5, Loss: 0.7638199329376221\n",
      "Epoch 7, Batch 6, Loss: 0.7713904976844788\n",
      "Epoch 7, Batch 7, Loss: 0.7527406811714172\n",
      "Epoch 7, Batch 8, Loss: 0.7406758069992065\n",
      "Epoch 7, Batch 9, Loss: 0.7526266574859619\n",
      "Epoch 7, Batch 10, Loss: 0.7562010884284973\n",
      "Epoch 7, Batch 11, Loss: 0.7334346175193787\n",
      "Epoch 7, Batch 12, Loss: 0.7460892200469971\n",
      "Epoch 7, Batch 13, Loss: 0.7459484338760376\n",
      "Epoch 7, Batch 14, Loss: 0.7633689045906067\n",
      "Epoch 7 completed.\n",
      "Epoch 8, Batch 1, Loss: 0.7429952621459961\n",
      "Epoch 8, Batch 2, Loss: 0.7027525901794434\n",
      "Epoch 8, Batch 3, Loss: 0.7612296938896179\n",
      "Epoch 8, Batch 4, Loss: 0.7154345512390137\n",
      "Epoch 8, Batch 5, Loss: 0.7183283567428589\n",
      "Epoch 8, Batch 6, Loss: 0.751353919506073\n",
      "Epoch 8, Batch 7, Loss: 0.7755038738250732\n",
      "Epoch 8, Batch 8, Loss: 0.7182281613349915\n",
      "Epoch 8, Batch 9, Loss: 0.7291672229766846\n",
      "Epoch 8, Batch 10, Loss: 0.7641505002975464\n",
      "Epoch 8, Batch 11, Loss: 0.7455236911773682\n",
      "Epoch 8, Batch 12, Loss: 0.712329626083374\n",
      "Epoch 8, Batch 13, Loss: 0.7692992091178894\n",
      "Epoch 8, Batch 14, Loss: 0.7448217272758484\n",
      "Epoch 8 completed.\n",
      "Epoch 9, Batch 1, Loss: 0.7433522343635559\n",
      "Epoch 9, Batch 2, Loss: 0.7367322444915771\n",
      "Epoch 9, Batch 3, Loss: 0.7302080392837524\n",
      "Epoch 9, Batch 4, Loss: 0.7204616069793701\n",
      "Epoch 9, Batch 5, Loss: 0.7337215542793274\n",
      "Epoch 9, Batch 6, Loss: 0.7524657249450684\n",
      "Epoch 9, Batch 7, Loss: 0.7239153385162354\n",
      "Epoch 9, Batch 8, Loss: 0.7294468879699707\n",
      "Epoch 9, Batch 9, Loss: 0.7650901675224304\n",
      "Epoch 9, Batch 10, Loss: 0.7330955862998962\n",
      "Epoch 9, Batch 11, Loss: 0.7209616303443909\n",
      "Epoch 9, Batch 12, Loss: 0.7375918626785278\n",
      "Epoch 9, Batch 13, Loss: 0.7224422693252563\n",
      "Epoch 9, Batch 14, Loss: 0.73720782995224\n",
      "Epoch 9 completed.\n",
      "Epoch 10, Batch 1, Loss: 0.719484269618988\n",
      "Epoch 10, Batch 2, Loss: 0.7394787073135376\n",
      "Epoch 10, Batch 3, Loss: 0.7688042521476746\n",
      "Epoch 10, Batch 4, Loss: 0.7133998870849609\n",
      "Epoch 10, Batch 5, Loss: 0.7312846779823303\n",
      "Epoch 10, Batch 6, Loss: 0.7594059705734253\n",
      "Epoch 10, Batch 7, Loss: 0.7252683043479919\n",
      "Epoch 10, Batch 8, Loss: 0.7253474593162537\n",
      "Epoch 10, Batch 9, Loss: 0.7267897725105286\n",
      "Epoch 10, Batch 10, Loss: 0.7310348153114319\n",
      "Epoch 10, Batch 11, Loss: 0.7182217836380005\n",
      "Epoch 10, Batch 12, Loss: 0.7373046875\n",
      "Epoch 10, Batch 13, Loss: 0.7140586972236633\n",
      "Epoch 10, Batch 14, Loss: 0.7149826884269714\n",
      "Epoch 10 completed.\n",
      "Epoch 11, Batch 1, Loss: 0.7072575092315674\n",
      "Epoch 11, Batch 2, Loss: 0.7417808771133423\n",
      "Epoch 11, Batch 3, Loss: 0.7192612886428833\n",
      "Epoch 11, Batch 4, Loss: 0.7217761874198914\n",
      "Epoch 11, Batch 5, Loss: 0.715881884098053\n",
      "Epoch 11, Batch 6, Loss: 0.6983544230461121\n",
      "Epoch 11, Batch 7, Loss: 0.7297123074531555\n",
      "Epoch 11, Batch 8, Loss: 0.753994345664978\n",
      "Epoch 11, Batch 9, Loss: 0.7420117855072021\n",
      "Epoch 11, Batch 10, Loss: 0.7239761352539062\n",
      "Epoch 11, Batch 11, Loss: 0.716740608215332\n",
      "Epoch 11, Batch 12, Loss: 0.7424871325492859\n",
      "Epoch 11, Batch 13, Loss: 0.7381557822227478\n",
      "Epoch 11, Batch 14, Loss: 0.7176022529602051\n",
      "Epoch 11 completed.\n",
      "Epoch 12, Batch 1, Loss: 0.7231312394142151\n",
      "Epoch 12, Batch 2, Loss: 0.7311784029006958\n",
      "Epoch 12, Batch 3, Loss: 0.7182464599609375\n",
      "Epoch 12, Batch 4, Loss: 0.727944016456604\n",
      "Epoch 12, Batch 5, Loss: 0.7446867227554321\n",
      "Epoch 12, Batch 6, Loss: 0.7059053182601929\n",
      "Epoch 12, Batch 7, Loss: 0.760862410068512\n",
      "Epoch 12, Batch 8, Loss: 0.7079306244850159\n",
      "Epoch 12, Batch 9, Loss: 0.7160336971282959\n",
      "Epoch 12, Batch 10, Loss: 0.7102786302566528\n",
      "Epoch 12, Batch 11, Loss: 0.6993359923362732\n",
      "Epoch 12, Batch 12, Loss: 0.73748779296875\n",
      "Epoch 12, Batch 13, Loss: 0.7044439315795898\n",
      "Epoch 12, Batch 14, Loss: 0.7265303730964661\n",
      "Epoch 12 completed.\n",
      "Epoch 13, Batch 1, Loss: 0.7310062050819397\n",
      "Epoch 13, Batch 2, Loss: 0.694464921951294\n",
      "Epoch 13, Batch 3, Loss: 0.7425933480262756\n",
      "Epoch 13, Batch 4, Loss: 0.7204216718673706\n",
      "Epoch 13, Batch 5, Loss: 0.7334489226341248\n",
      "Epoch 13, Batch 6, Loss: 0.7039212584495544\n",
      "Epoch 13, Batch 7, Loss: 0.6976197957992554\n",
      "Epoch 13, Batch 8, Loss: 0.7456589937210083\n",
      "Epoch 13, Batch 9, Loss: 0.6716532707214355\n",
      "Epoch 13, Batch 10, Loss: 0.7297869920730591\n",
      "Epoch 13, Batch 11, Loss: 0.7196992635726929\n",
      "Epoch 13, Batch 12, Loss: 0.7334640622138977\n",
      "Epoch 13, Batch 13, Loss: 0.7410314679145813\n",
      "Epoch 13, Batch 14, Loss: 0.689988374710083\n",
      "Epoch 13 completed.\n",
      "Epoch 14, Batch 1, Loss: 0.6943323612213135\n",
      "Epoch 14, Batch 2, Loss: 0.7355098128318787\n",
      "Epoch 14, Batch 3, Loss: 0.7245577573776245\n",
      "Epoch 14, Batch 4, Loss: 0.7078335285186768\n",
      "Epoch 14, Batch 5, Loss: 0.7157537341117859\n",
      "Epoch 14, Batch 6, Loss: 0.7212860584259033\n",
      "Epoch 14, Batch 7, Loss: 0.7068639397621155\n",
      "Epoch 14, Batch 8, Loss: 0.7243831753730774\n",
      "Epoch 14, Batch 9, Loss: 0.7138020992279053\n",
      "Epoch 14, Batch 10, Loss: 0.7179670929908752\n",
      "Epoch 14, Batch 11, Loss: 0.713638186454773\n",
      "Epoch 14, Batch 12, Loss: 0.7117232084274292\n",
      "Epoch 14, Batch 13, Loss: 0.7160851359367371\n",
      "Epoch 14, Batch 14, Loss: 0.6996312141418457\n",
      "Epoch 14 completed.\n",
      "Epoch 15, Batch 1, Loss: 0.7122482061386108\n",
      "Epoch 15, Batch 2, Loss: 0.6913407444953918\n",
      "Epoch 15, Batch 3, Loss: 0.728985607624054\n",
      "Epoch 15, Batch 4, Loss: 0.6968536376953125\n",
      "Epoch 15, Batch 5, Loss: 0.6999648213386536\n",
      "Epoch 15, Batch 6, Loss: 0.691571056842804\n",
      "Epoch 15, Batch 7, Loss: 0.6976590156555176\n",
      "Epoch 15, Batch 8, Loss: 0.7064395546913147\n",
      "Epoch 15, Batch 9, Loss: 0.7287301421165466\n",
      "Epoch 15, Batch 10, Loss: 0.7032326459884644\n",
      "Epoch 15, Batch 11, Loss: 0.7301129102706909\n",
      "Epoch 15, Batch 12, Loss: 0.7214168906211853\n",
      "Epoch 15, Batch 13, Loss: 0.7069271802902222\n",
      "Epoch 15, Batch 14, Loss: 0.7477597594261169\n",
      "Epoch 15 completed.\n",
      "Epoch 16, Batch 1, Loss: 0.699160099029541\n",
      "Epoch 16, Batch 2, Loss: 0.7385075688362122\n",
      "Epoch 16, Batch 3, Loss: 0.7293450832366943\n",
      "Epoch 16, Batch 4, Loss: 0.6750197410583496\n",
      "Epoch 16, Batch 5, Loss: 0.7439712285995483\n",
      "Epoch 16, Batch 6, Loss: 0.7106679677963257\n",
      "Epoch 16, Batch 7, Loss: 0.7165948748588562\n",
      "Epoch 16, Batch 8, Loss: 0.7155913710594177\n",
      "Epoch 16, Batch 9, Loss: 0.6977366805076599\n",
      "Epoch 16, Batch 10, Loss: 0.6852360367774963\n",
      "Epoch 16, Batch 11, Loss: 0.6923781633377075\n",
      "Epoch 16, Batch 12, Loss: 0.7229127883911133\n",
      "Epoch 16, Batch 13, Loss: 0.6838791966438293\n",
      "Epoch 16, Batch 14, Loss: 0.6949222087860107\n",
      "Epoch 16 completed.\n",
      "Epoch 17, Batch 1, Loss: 0.6920282244682312\n",
      "Epoch 17, Batch 2, Loss: 0.6934365630149841\n",
      "Epoch 17, Batch 3, Loss: 0.6963820457458496\n",
      "Epoch 17, Batch 4, Loss: 0.701718807220459\n",
      "Epoch 17, Batch 5, Loss: 0.7104454040527344\n",
      "Epoch 17, Batch 6, Loss: 0.7218654155731201\n",
      "Epoch 17, Batch 7, Loss: 0.7062579393386841\n",
      "Epoch 17, Batch 8, Loss: 0.6823629140853882\n",
      "Epoch 17, Batch 9, Loss: 0.7340244650840759\n",
      "Epoch 17, Batch 10, Loss: 0.6988674402236938\n",
      "Epoch 17, Batch 11, Loss: 0.6935272812843323\n",
      "Epoch 17, Batch 12, Loss: 0.7134191989898682\n",
      "Epoch 17, Batch 13, Loss: 0.7037632465362549\n",
      "Epoch 17, Batch 14, Loss: 0.7117033004760742\n",
      "Epoch 17 completed.\n",
      "Epoch 18, Batch 1, Loss: 0.7031142115592957\n",
      "Epoch 18, Batch 2, Loss: 0.6842268109321594\n",
      "Epoch 18, Batch 3, Loss: 0.672667920589447\n",
      "Epoch 18, Batch 4, Loss: 0.7297580242156982\n",
      "Epoch 18, Batch 5, Loss: 0.694410502910614\n",
      "Epoch 18, Batch 6, Loss: 0.7072924971580505\n",
      "Epoch 18, Batch 7, Loss: 0.7134556770324707\n",
      "Epoch 18, Batch 8, Loss: 0.6981874704360962\n",
      "Epoch 18, Batch 9, Loss: 0.6936026811599731\n",
      "Epoch 18, Batch 10, Loss: 0.696902334690094\n",
      "Epoch 18, Batch 11, Loss: 0.7114241719245911\n",
      "Epoch 18, Batch 12, Loss: 0.7133356332778931\n",
      "Epoch 18, Batch 13, Loss: 0.6967933773994446\n",
      "Epoch 18, Batch 14, Loss: 0.6910213232040405\n",
      "Epoch 18 completed.\n",
      "Epoch 19, Batch 1, Loss: 0.6818154454231262\n",
      "Epoch 19, Batch 2, Loss: 0.6710174679756165\n",
      "Epoch 19, Batch 3, Loss: 0.7006118893623352\n",
      "Epoch 19, Batch 4, Loss: 0.7048524022102356\n",
      "Epoch 19, Batch 5, Loss: 0.7172785997390747\n",
      "Epoch 19, Batch 6, Loss: 0.6867824196815491\n",
      "Epoch 19, Batch 7, Loss: 0.7084425687789917\n",
      "Epoch 19, Batch 8, Loss: 0.6809315085411072\n",
      "Epoch 19, Batch 9, Loss: 0.7031269073486328\n",
      "Epoch 19, Batch 10, Loss: 0.722716212272644\n",
      "Epoch 19, Batch 11, Loss: 0.7115590572357178\n",
      "Epoch 19, Batch 12, Loss: 0.7085036635398865\n",
      "Epoch 19, Batch 13, Loss: 0.6650971174240112\n",
      "Epoch 19, Batch 14, Loss: 0.7013685703277588\n",
      "Epoch 19 completed.\n",
      "Epoch 20, Batch 1, Loss: 0.7109005451202393\n",
      "Epoch 20, Batch 2, Loss: 0.6727211475372314\n",
      "Epoch 20, Batch 3, Loss: 0.6976675391197205\n",
      "Epoch 20, Batch 4, Loss: 0.7296550273895264\n",
      "Epoch 20, Batch 5, Loss: 0.6911771893501282\n",
      "Epoch 20, Batch 6, Loss: 0.6970698237419128\n",
      "Epoch 20, Batch 7, Loss: 0.7079198956489563\n",
      "Epoch 20, Batch 8, Loss: 0.6681689023971558\n",
      "Epoch 20, Batch 9, Loss: 0.6914381384849548\n",
      "Epoch 20, Batch 10, Loss: 0.706007182598114\n",
      "Epoch 20, Batch 11, Loss: 0.6666540503501892\n",
      "Epoch 20, Batch 12, Loss: 0.6888441443443298\n",
      "Epoch 20, Batch 13, Loss: 0.6697809100151062\n",
      "Epoch 20, Batch 14, Loss: 0.7251085638999939\n",
      "Epoch 20 completed.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "epochs = 20\n",
    "pad_token_label_id = -100\n",
    "\n",
    "def get_embeddings(tokens):\n",
    "    inputs = tokenizer(tokens, return_tensors=\"pt\", is_split_into_words=True, padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    return outputs.last_hidden_state, inputs['attention_mask']  # 返回 embeddings 和 attention_mask\n",
    "\n",
    "def pad_labels(labels, max_length, pad_token_label_id):\n",
    "    \"\"\"对 labels 进行 padding，长度补齐到 max_length\"\"\"\n",
    "    labels = labels + [pad_token_label_id] * (max_length - len(labels))\n",
    "    return labels\n",
    "\n",
    "# 自定义 collate_fn 函数来对 tokens 和 labels 进行 padding\n",
    "def collate_fn(batch):\n",
    "    tokens = [item['tokens'] for item in batch]\n",
    "    labels = [item['ner_tags'] for item in batch]\n",
    "\n",
    "    max_length = max(len(token_list) for token_list in tokens)\n",
    "\n",
    "    padded_tokens = [token_list + [''] * (max_length - len(token_list)) for token_list in tokens]\n",
    "    padded_labels = [label_list + [pad_token_label_id] * (max_length - len(label_list)) for label_list in labels]\n",
    "\n",
    "    return {'tokens': padded_tokens, 'ner_tags': padded_labels}\n",
    "\n",
    "batch_size = 1024\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# 开始训练\n",
    "for epoch in range(epochs):\n",
    "    batch_counter = 0\n",
    "    for batch in train_dataloader:\n",
    "        tokens = batch['tokens']\n",
    "        labels = batch['ner_tags']\n",
    "\n",
    "        # 获取 tokens 的 embeddings 和 attention mask\n",
    "        embeddings, attention_mask = get_embeddings(tokens)\n",
    "\n",
    "        # 对 labels 进行 padding，长度与 embeddings 的 sequence length 匹配\n",
    "        padded_labels = [pad_labels(label_list, embeddings.size(1), pad_token_label_id) for label_list in labels]\n",
    "        padded_labels = torch.tensor(padded_labels).to(device)  # 将 labels 转移到 GPU\n",
    "\n",
    "        # 转换为 tensor 并展平\n",
    "        outputs = model(embeddings)\n",
    "        outputs = outputs.view(-1, num_labels)  # 将输出展平为 [batch_size * seq_length, num_labels]\n",
    "\n",
    "        padded_labels = padded_labels.view(-1)  # 将 labels 展平为 [batch_size * seq_length]\n",
    "\n",
    "        # 计算损失\n",
    "        loss = loss_fn(outputs, padded_labels)\n",
    "\n",
    "        # 反向传播并更新权重\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 输出当前 batch 计数器和损失值\n",
    "        batch_counter += 1\n",
    "        print(f'Epoch {epoch+1}, Batch {batch_counter}, Loss: {loss.item()}')\n",
    "\n",
    "    print(f'Epoch {epoch+1} completed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2457eb2b-9c92-43c3-8b7d-a5272a112af6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
