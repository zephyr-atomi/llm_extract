{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95d10d31-a282-4c03-b817-26f059af7cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lamuguo/venv/dbpedia/lib/python3.12/site-packages/datasets/load.py:1454: FutureWarning: The repository for conll2003 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/conll2003\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: EU, Embedding: [ 0.0328505   0.04594225  0.00482348 -0.0299531  -0.0306954 ]...\n",
      "Token: rejects, Embedding: [-0.03228948  0.06037299  0.05513505  0.06366471  0.03532343]...\n",
      "Token: German, Embedding: [-0.01822809  0.03050454  0.00161921  0.05627387 -0.01692749]...\n",
      "Token: call, Embedding: [-0.09879501  0.03357653 -0.04692755 -0.0002789  -0.07271501]...\n",
      "Token: to, Embedding: [-0.02195787  0.042925   -0.0413069   0.08042946 -0.01573347]...\n",
      "Token: boycott, Embedding: [ 0.01419001  0.07401178  0.06483291 -0.04102125  0.04765184]...\n",
      "Token: British, Embedding: [ 0.0284378  -0.01627326 -0.01693945 -0.00372896 -0.01063101]...\n",
      "Token: lamb, Embedding: [-0.08062177  0.01720387 -0.01789038  0.08488934 -0.05845075]...\n",
      "Token: ., Embedding: [-0.13382298  0.01415094 -0.01621612 -0.02662739  0.06019066]...\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 加载 CoNLL 2003 数据集的训练集\n",
    "dataset = load_dataset(\"conll2003\")\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "# 加载 Sentence-BERT 模型\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# 为每个句子的 tokens 生成 embeddings\n",
    "token_embeddings = []\n",
    "\n",
    "for sample in train_dataset:\n",
    "    tokens = sample['tokens']\n",
    "    \n",
    "    # 生成该句子所有 tokens 的 embeddings\n",
    "    embeddings = model.encode(tokens)\n",
    "    \n",
    "    # 保存嵌入\n",
    "    token_embeddings.append(embeddings)\n",
    "\n",
    "# 打印第一个句子的 tokens 和它们对应的 embeddings\n",
    "for token, embedding in zip(train_dataset[0]['tokens'], token_embeddings[0]):\n",
    "    print(f\"Token: {token}, Embedding: {embedding[:5]}...\")  # 打印前5个值，简化输出\n",
    "\n",
    "# 检查 token_embeddings 的类型和形状\n",
    "import numpy as np\n",
    "\n",
    "# 将 token_embeddings 转换为 numpy 数组\n",
    "# 注意：token_embeddings 是一个嵌套列表，需要将它展平成二维数组\n",
    "token_embeddings = np.vstack(token_embeddings)\n",
    "\n",
    "# 检查转换后的形状\n",
    "print(f\"Shape of token_embeddings after conversion: {token_embeddings.shape}\")\n",
    "\n",
    "print(f\"Type of token_embeddings: {type(token_embeddings)}\")\n",
    "print(f\"Shape of token_embeddings: {token_embeddings.shape if isinstance(token_embeddings, np.ndarray) else 'Not a numpy array'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86a3667d-75f9-47ba-8928-1547786dc6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clustering embeddings:   0%|                                     | 0/203621 [2:25:23<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 5309\n",
      "Number of noise points: 32114\n"
     ]
    }
   ],
   "source": [
    "# 执行 HDBSCAN 聚类\n",
    "cluster_labels = clusterer.fit_predict(token_embeddings)\n",
    "\n",
    "# 2. 聚类结果输出\n",
    "num_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "num_noise_points = list(cluster_labels).count(-1)\n",
    "\n",
    "print(f\"Number of clusters: {num_clusters}\")\n",
    "print(f\"Number of noise points: {num_noise_points}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
